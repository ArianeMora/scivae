{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST example using scivae\n",
    "\n",
    "Here we just show a simple example of how the data we can encode & predict from the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scivae import Optimiser, VAE, Validate\n",
    "from sciviso import Scatterplot\n",
    "\n",
    "\n",
    "\n",
    "# Set the location of the mnist data\n",
    "data_dir = '../tests/data/mnist/'\n",
    "image_size = 28\n",
    "# The more images/data you have the better it will be\n",
    "num_images = 40000\n",
    "\n",
    "# Set up the data format (you don't normally need to do this (i.e. a normal np array works best))\n",
    "test_f = open(f'{data_dir}train-images-idx3-ubyte', 'rb')\n",
    "test_f.read(16)\n",
    "buf = test_f.read(image_size * image_size * num_images)\n",
    "test_data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "test_data = test_data.reshape(num_images, image_size * image_size)\n",
    "\n",
    "# Read in teh training labels\n",
    "f = open(f'{data_dir}train-labels-idx1-ubyte', 'rb')\n",
    "f.read(8)\n",
    "test_labels = []\n",
    "for i in range(0, len(test_data)):\n",
    "    buf = f.read(1)\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    test_labels.append(labels[0])\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The configuration is the design of the neural network, it has the following parameters:  \n",
    "\n",
    "1. loss: this is the metric for the loss function, and it can be: 'mse'=mean squared error (for continuous data), or 'ce' for cross entropy (for binary data)\n",
    "2. distance_metric: this is the distance metric between the distributions and can either be 'mmd': maximum mean discrepency, or kl: Kullbackâ€“Leibler (see this blog for details: https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/)\n",
    "3. mmd_weight: this is how much you want to force the VAE to learn \n",
    "config = {'loss': {'loss_type': 'mse', 'distance_metric': 'mmd', 'mmd_weight': 1}, \n",
    "          'encoding': {'layers': [{'num_nodes': 128, 'activation_fn': 'selu'},\n",
    "                                  {'num_nodes': 64, 'activation_fn': 'relu'}]}, \n",
    "          'decoding': {'layers': [{'num_nodes': 64, 'activation_fn': 'relu'}, \n",
    "                                  {'num_nodes': 128, 'activation_fn': 'selu'}]}, \n",
    "          'latent': {'num_nodes': 2}, 'optimiser': {'params': {}, 'name': 'adam'}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Create a configuration for the dataset\n",
    "config = {'scale_data': True,\n",
    "          'loss': {'loss_type': 'mse', 'distance_metric': 'mmd', 'mmd_weight': 1}, \n",
    "          'encoding': {'layers': [{'num_nodes': 128, 'activation_fn': 'selu'},\n",
    "                                  {'num_nodes': 64, 'activation_fn': 'relu'}]}, \n",
    "          'decoding': {'layers': [{'num_nodes': 64, 'activation_fn': 'relu'}, \n",
    "                                  {'num_nodes': 128, 'activation_fn': 'selu'}]}, \n",
    "          'latent': {'num_nodes': 2}, 'optimiser': {'params': {}, 'name': 'adagrad'}}\n",
    "\n",
    "\n",
    "\n",
    "# Run the VAE \n",
    "vae = VAE(test_data, test_data, test_labels, config, 'vae')\n",
    "# The more epochs you run the better (until some point so would recomend more though it wil take longer)\n",
    "vae.encode('default', epochs=100, batch_size=1000)\n",
    "# get the encoded data\n",
    "encoding = vae.get_encoded_data()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "encoding = vae.get_encoded_data()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the reconstruction\n",
    "\n",
    "\n",
    "Have a look at how good the reconstruction is from the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Show some of the images (4)\n",
    "n = 4\n",
    "\n",
    "# Show the true data\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(test_data[i].reshape(28, 28))\n",
    "\n",
    "plt.show()\n",
    "# Ensure we're encoding and decoding the same dataset\n",
    "scaler = MinMaxScaler(copy=True)\n",
    "test_data_scaled = scaler.fit_transform(test_data)\n",
    "# Show the prediction of the test data (i.e. using the test data how does it get reconstructed)\n",
    "\n",
    "encoding = vae.encode_new_data(test_data_scaled)\n",
    "# Show the prediction of the test data (i.e. using the test data how does it get reconstructed)\n",
    "d = vae.decoder.predict(encoding)\n",
    "plt.figure(figsize=(20, 2))\n",
    "n = 4\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(d[i,:].reshape(28, 28))\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the latent space\n",
    "\n",
    "Here we can have a look at the latent space by simply plotting the latent space & looking at how our labels separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# Plot the latent space - have a look at the first two latent nodes\n",
    "vis_df = pd.DataFrame()\n",
    "vis_df['latent_0'] = encoding[:, 0]\n",
    "vis_df['latent_1'] = encoding[:, 1]\n",
    "vis_df['test_label'] = test_labels\n",
    "\n",
    "\n",
    "# Zip the colours to make the labels into numeric values\n",
    "lut = dict(zip(set(test_labels), sns.color_palette(\"Set1\", len(set(test_labels)))))\n",
    "row_colors2 = pd.DataFrame(test_labels)[0].map(lut)\n",
    "vis_df['label'] = row_colors2\n",
    "scatter = Scatterplot(vis_df, 'latent_0', 'latent_1',  \n",
    "                      colour=vis_df['test_label'].values, title='Coloured by labels', \n",
    "                     add_legend=True)\n",
    "scatter.plot()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
